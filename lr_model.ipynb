{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Nh·∫≠n di·ªán c·∫£m x√∫c t·ª´ √¢m thanh\n",
    "S·ª≠ d·ª•ng Logistic Regression"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## I. Import th∆∞ vi·ªán c·∫ßn thi·∫øt"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "# pandas and numpy for data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# os and sys for file and system path handling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a library for audio processing, used here to extract features from audio files\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# seaborn and matplotlib for data visualization (plots)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn libraries for preprocessing and splitting data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn libraries for generating confusion matrix and detailed classification report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Seaborn library for visualization of the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# IPython's Audio class for playing audio files directly in Jupyter\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Suppress warnings to avoid cluttering the output\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## II. Chu·∫©n b·ªã d·ªØ li·ªáu - Data Preparation\n",
    "Khai b√°o ƒë∆∞·ªùng d·∫´n ƒë·∫øn c√°c t·∫≠p d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define dataset paths\n",
    "\n",
    "# RAVDESS: Ryerson Audio-Visual Database of Emotional Speech and Song\n",
    "Ravdess = \"data/ravdess-emotional-speech-audio/audio_speech_actors_01-24/archive/\"\n",
    "\n",
    "# CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset\n",
    "Crema = \"data/cremad/AudioWAV/\"\n",
    "\n",
    "# TESS: Toronto Emotional Speech Set\n",
    "Tess = \"data/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "\n",
    "# SAVEE: Surrey Audio-Visual Expressed Emotion\n",
    "Savee = \"data/surrey-audiovisual-expressed-emotion-savee/ALL/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     1. RAVDESS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of subdirectories in the RAVDESS dataset directory\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []  # List to store the emotion label for each audio file\n",
    "file_path = []  # List to store the full path of each audio file\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    if dir == '.DS_Store':  # Skip MacOS system file\n",
    "        continue\n",
    "    # Each subdirectory corresponds to an actor and contains audio files\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]  # Remove the file extension (.wav)\n",
    "        part = part.split('-')  # Split the filename into parts based on RAVDESS naming convention\n",
    "\n",
    "        # The third element in the filename represents the emotion\n",
    "        file_emotion.append(int(part[2]))\n",
    "\n",
    "        # Construct and save the full path to the audio file\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the audio file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Map numeric emotion codes to actual emotion labels\n",
    "Ravdess_df.Emotions.replace(\n",
    "    {\n",
    "        1: 'neutral',\n",
    "        2: 'calm',\n",
    "        3: 'happy',\n",
    "        4: 'sad',\n",
    "        5: 'angry',\n",
    "        6: 'fear',\n",
    "        7: 'disgust',\n",
    "        8: 'surprise'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "Ravdess_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     2. CREMA-D"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of all audio files in the CREMA-D dataset directory\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store file paths\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # Append the full file path to the list\n",
    "    file_path.append(Crema + file)\n",
    "\n",
    "    # Extract emotion code from the file name (e.g., '1001_DFA_ANG_XX.wav')\n",
    "    part = file.split('_')\n",
    "\n",
    "    # Map the emotion code to the corresponding emotion label\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')  # Fallback in case of unexpected code\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine emotion and path into one DataFrame\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "Crema_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     3. TESS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of speaker folders in the TESS dataset directory\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store full file paths\n",
    "\n",
    "# Iterate through each speaker's folder\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)  # Get list of audio files in the speaker folder\n",
    "    for file in directories:\n",
    "        # Extract the emotion from the file name (e.g., 'OAF_angry.wav')\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]  # The third segment contains the emotion label\n",
    "\n",
    "        # Handle special case: 'ps' stands for 'pleasant surprise'\n",
    "        if part == 'ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "\n",
    "        # Append the full file path\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "\n",
    "# Create DataFrame for extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create DataFrame for corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine both into a single DataFrame\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Preview the result\n",
    "Tess_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     4. SAVEE"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of all audio files in the SAVEE dataset directory\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store full file paths\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)  # Construct full file path\n",
    "\n",
    "    # Extract the emotion code from the filename\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]  # Remove the numerical part and extension\n",
    "\n",
    "    # Map short emotion codes to full labels\n",
    "    if ele == 'a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele == 'd':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele == 'f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele == 'h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele == 'n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele == 'sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Preview the result\n",
    "Savee_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### => K·∫øt h·ª£p c√°c t·∫≠p d·ªØ li·ªáu ƒë√£ t·∫£i"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine datasets from all four sources (RAVDESS, CREMA-D, TESS, SAVEE)\n",
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\n",
    "\n",
    "# Xo√° tr·ªôn tr√°nh bias\n",
    "data_path = data_path.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file for future use\n",
    "data_path.to_csv(\"data_path.csv\", index=False)\n",
    "\n",
    "# Preview the first few rows of the combined dataset\n",
    "data_path.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## üìäBi·ªÉu ƒë·ªì s·ªë l∆∞·ª£ng m·∫´u cho m·ªói c·∫£m x√∫c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plotting the number of samples for each emotion\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set figure size for better readability\n",
    "\n",
    "# Create a count plot for the 'Emotions' column\n",
    "sns.countplot(x='Emotions', data=data_path, palette='muted')\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Count of Emotions', fontsize=16)\n",
    "plt.xlabel('Emotions', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Remove top and right borders for a cleaner look\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "\n",
    "# Automatically adjust subplot parameters to fit the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the final plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## III. Kh√°m ph√° v√† tr·ª±c quan d·ªØ li·ªáu - (Exploratory Data Analysis - EDA):\n",
    "##### 1. Bi·ªÉu ƒë·ªì d·∫°ng s√≥ng (waveform).\n",
    "##### 2. Quang ph·ªï √¢m thanh (spectrogram) - bi·ªÉu di·ªÖn t√≠n hi·ªáu √¢m thanh trong mi·ªÅn t·∫ßn s·ªë theo th·ªùi gian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to create a waveplot for a given emotion and audio data\n",
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))  # Set figure size\n",
    "    plt.title(f'Waveplot for audio with {e} emotion', size=15)  # Title of the plot\n",
    "    plt.plot(np.linspace(0, len(data) / sr, num=len(data)), data, alpha=0.7)  # Plot audio data over time\n",
    "    plt.xlabel(\"Time (s)\")  # X-axis label\n",
    "    plt.ylabel(\"Amplitude\")  # Y-axis label\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "# Function to create a spectrogram for a given emotion and audio data\n",
    "def create_spectrogram(data, sr, e):\n",
    "    # Compute the Short-Time Fourier Transform (STFT)\n",
    "    X = librosa.stft(data)\n",
    "    # Convert amplitude to decibels\n",
    "    Xdb = librosa.amplitude_to_db(np.abs(X))\n",
    "    plt.figure(figsize=(12, 3))  # Set figure size\n",
    "    plt.title(f'Spectrogram for audio with {e} emotion', size=15)  # Title of the plot\n",
    "    # Display the spectrogram with the amplitude in decibels\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz', cmap='magma')\n",
    "    plt.colorbar(format=\"%+2.0f dB\")  # Add color bar\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    plt.show()  # Display the plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_emotion(emotion, data_path):\n",
    "    try:\n",
    "        # L·∫•y file ƒë·∫ßu ti√™n c√≥ emotion t∆∞∆°ng ·ª©ng\n",
    "        path = np.array(data_path.Path[data_path.Emotions == emotion])[0]\n",
    "\n",
    "        # Load audio\n",
    "        data, sampling_rate = librosa.load(path, sr=None)\n",
    "\n",
    "        # V·∫Ω s√≥ng √¢m thanh\n",
    "        create_waveplot(data, sampling_rate, emotion)\n",
    "\n",
    "        # V·∫Ω spectrogram\n",
    "        create_spectrogram(data, sampling_rate, emotion)\n",
    "\n",
    "        # Ph√°t audio trong notebook\n",
    "        display(Audio(path))\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"No audio files found for emotion: {emotion}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualize_emotion('happy', data_path)\n",
    "\n",
    "visualize_emotion('sad', data_path)\n",
    "\n",
    "visualize_emotion('fear', data_path)\n",
    "\n",
    "visualize_emotion('angry', data_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IV. M·ªü r·ªông d·ªØ li√™u - Data Augmentation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Augmentation functions\n",
    "\n",
    "def noise(data):\n",
    "    # Generate random noise amplitude based on the maximum value of the data\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Random amplitude of noise\n",
    "    # Add Gaussian noise to the original signal\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])  # Apply noise to the audio data\n",
    "    return data\n",
    "\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    # Ensure that data is a numpy array and of type float32 for time-stretching\n",
    "    return librosa.effects.time_stretch(np.array(data).astype(np.float32),\n",
    "                                        rate=rate)  # Stretch the audio signal in time\n",
    "\n",
    "\n",
    "def shift(data):\n",
    "    # Generate a random shift range for shifting the audio signal in time\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)  # Random shift in milliseconds\n",
    "    # Shift the audio data by the computed range using np.roll\n",
    "    return np.roll(data, shift_range)  # Shift the audio data in time\n",
    "\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    # Ensure that data is a numpy array and of type float32 for pitch shifting\n",
    "    data = np.array(data)\n",
    "    # Apply pitch shift by a factor (in steps), which will change the pitch of the audio signal\n",
    "    return librosa.effects.pitch_shift(data.astype(np.float32), sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "\n",
    "# Taking any example from the dataset and testing the augmentation functions\n",
    "path = np.array(data_path.Path)[1]  # Example audio file path\n",
    "data, sample_rate = librosa.load(path, sr=None)  # Load the audio file and obtain its data and sample rate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. V√≠ d·ª• ƒë∆°n gi·∫£n tr·ª±c quan ho√° √¢m thanh theo d·∫°ng s√≥ng\n",
    "-> Ki·ªÉm tra v√† ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ƒë·∫ßu v√†o."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simple audio waveform visualization\n",
    "\n",
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the audio signal data as a waveform.\n",
    "# `np.linspace` is used to create time values based on the number of samples and the sample rate.\n",
    "plt.plot(np.linspace(0, len(data) / sample_rate, num=len(data)), data, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title(\"Waveform\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\"\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\"\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Automatically adjust the layout to fit everything properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the audio file using IPython's Audio class\n",
    "Audio(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Noise Injection Visualization (th√™m nhi·ªÖu v√†o t√≠n hi·ªáu g·ªëc)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Noise Injection: Adding random noise to the audio signal\n",
    "\n",
    "# Inject noise into the original audio data\n",
    "x = noise(data)\n",
    "\n",
    "# Create a figure for plotting the noisy signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the noisy signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title(\"Noise Injection\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the noisy audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Time Stretching Visualization - Thay ƒë·ªïi t·ªëc ƒë·ªô ph√°t √¢m m√† kh√¥ng l√†m thay ƒë·ªïi cao ƒë·ªô (pitch)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time Stretching: Changing the speed of the audio signal without altering its pitch\n",
    "\n",
    "# Apply time-stretching to the original audio data\n",
    "x = stretch(data)\n",
    "\n",
    "# Create a figure for plotting the stretched signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the stretched signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for time-stretching\n",
    "plt.title(\"Time Stretching\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the time-stretched audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.Time Shifting Visualization ‚Äì D·ªãch chuy·ªÉn t√≠n hi·ªáu √¢m thanh theo th·ªùi gian."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time Shifting: Shifting the audio signal in time (delays or advances the signal)\n",
    "\n",
    "# Apply time-shifting to the original audio data\n",
    "x = shift(data)\n",
    "\n",
    "# Create a figure for plotting the shifted signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the shifted signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for time-shifting\n",
    "plt.title(\"Time Shifting\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the time-shifted audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Pitch Shifting Visualization ‚Äî Thay ƒë·ªïi cao ƒë·ªô (pitch) c·ªßa t√≠n hi·ªáu √¢m thanh m√† kh√¥ng thay ƒë·ªïi t·ªëc ƒë·ªô ph√°t."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pitch Shifting: Changing the pitch (frequency) of the audio signal\n",
    "\n",
    "# Apply pitch-shifting to the original audio data\n",
    "x = pitch(data, sample_rate)\n",
    "\n",
    "# Create a figure for plotting the pitch-shifted signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the pitch-shifted signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for pitch-shifting\n",
    "plt.title(\"Pitch Shifting\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the pitch-shifted audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## V. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng - Feature Extraction"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature Extraction (updated for newer APIs)\n",
    "def extract_features(data):\n",
    "    \"\"\"\n",
    "    This function extracts various audio features from the given audio data.\n",
    "    Features include zero-crossing rate (ZCR), chroma_stft, MFCC, RMS, and mel spectrogram.\n",
    "    \"\"\"\n",
    "    result = np.array([])  # Initialize an empty array to store features\n",
    "\n",
    "    # Zero-Crossing Rate (ZCR): Measure of how many times the signal changes sign\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))  # Append the ZCR feature to result array\n",
    "\n",
    "    # Chroma Short-Time Fourier Transform (chroma_stft): Measures harmonic and melodic content\n",
    "    stft = np.abs(librosa.stft(data))  # Short-time Fourier transform of the audio signal\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)  # Compute chroma features\n",
    "    result = np.hstack((result, chroma_stft))  # Append the chroma feature to result array\n",
    "\n",
    "    # Mel-frequency Cepstral Coefficients (MFCC): Represent the short-term power spectrum of sound\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T, axis=0)  # Extract 13 MFCCs\n",
    "    result = np.hstack((result, mfcc))  # Append the MFCC feature to result array\n",
    "\n",
    "    # Root Mean Square (RMS): Energy of the audio signal\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)  # Calculate RMS value\n",
    "    result = np.hstack((result, rms))  # Append the RMS feature to result array\n",
    "\n",
    "    # Mel Spectrogram: Spectrogram representation of the audio signal using Mel scale\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)  # Extract Mel spectrogram\n",
    "    result = np.hstack((result, mel))  # Append the Mel spectrogram feature to result array\n",
    "\n",
    "    return result  # Return the array of extracted features\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    This function loads the audio data from the given path, extracts features from the original data,\n",
    "    and applies data augmentation techniques (noise, stretching, and pitch shifting) to generate additional features.\n",
    "    \"\"\"\n",
    "    # Load the audio file (duration and offset are specified to load a portion of the audio)\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "\n",
    "    # Base features: Extract features from the original audio data\n",
    "    res1 = extract_features(data)\n",
    "    result = np.array(res1)  # Store base features as the result\n",
    "\n",
    "    # Noise Augmentation: Apply noise injection and extract features from the noisy audio\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, res2))  # Append noisy features to the result\n",
    "\n",
    "    # Stretching and Pitch Augmentation: Apply time-stretching and pitch-shifting, then extract features\n",
    "    new_data = stretch(data)  # Apply time-stretching\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)  # Apply pitch shifting\n",
    "    res3 = extract_features(data_stretch_pitch)  # Extract features from the stretched and pitch-shifted data\n",
    "    result = np.vstack((result, res3))  # Append augmented features to the result\n",
    "\n",
    "    return result  # Return the final feature set as a 2D array"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cho to√†n b·ªô dataset √¢m thanh k√®m theo nh√£n c·∫£m x√∫c t∆∞∆°ng ·ª©ng"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract features for the entire dataset\n",
    "X, Y = [], []  # Initialize lists to hold features (X) and labels (Y)\n",
    "\n",
    "# Loop through each audio file and its associated emotion\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "\n",
    "    # Extract features for the current audio file (including augmented variants)\n",
    "    features = get_features(path)  # Extract features for the original audio and its augmented versions\n",
    "\n",
    "    # Loop through the extracted features (original, noisy, stretched + pitched) and append them to X\n",
    "    for feature in features:\n",
    "        X.append(feature)  # Append the feature to the feature list (X)\n",
    "        Y.append(emotion)  # Append the corresponding emotion label to the label list (Y)\n",
    "\n",
    "        # Repeat 3 times because we have 3 variants (original, noise, and stretch + pitch)\n",
    "        # This ensures that for each audio file, we have 3 corresponding entries in X and Y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Ki·ªÉm tra s·ªë: M·∫´u ƒë·∫∑c tr∆∞ng (X) v√† s·ªë nh√£n t∆∞∆°ng ·ª©ng v·ªõi c√°c m·∫´u (Y)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the length of the feature set (X), the labels (Y), and the number of paths in data_path\n",
    "len(X), len(Y), data_path.Path.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### L∆∞u tr·ªØ d·ªØ li·ªáu ƒë·∫∑c tr∆∞ng v√† nh√£n c·ªßa to√†n b·ªô dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the features and labels into a DataFrame\n",
    "Features = pd.DataFrame(X)\n",
    "Features['labels'] = Y\n",
    "\n",
    "# Save to a CSV file\n",
    "Features.to_csv('features.csv', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "Features.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chu·∫©n b·ªã d·ªØ li·ªáu ƒë·∫∑c tr∆∞ng v√† nh√£n"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Features\n",
    "X = Features.iloc[:, :-1].values  # Extract all columns except the last (labels)\n",
    "\n",
    "# Labels\n",
    "Y = Features['labels'].values  # Extract the labels column"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### M√£ h√≥a nh√£n d·∫°ng ph√¢n lo·∫°i th√†nh d·∫°ng vector one-hot (chu·∫©n h√≥a cho b√†i to√°n ph√¢n lo·∫°i ƒëa l·ªõp)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# One-hot Encoding for labels\n",
    "# As this is a multiclass classification problem, we are one-hot encoding the labels (Y).\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Convert labels (Y) to a 2D array (required by OneHotEncoder) and apply fit_transform.\n",
    "# This will convert each categorical label into a binary vector (one-hot encoded).\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1, 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting data into training and testing sets\n",
    "# We use train_test_split from scikit-learn to split the data (X) and labels (Y)\n",
    "# into training and testing sets. The random_state ensures reproducibility of the split.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
    "\n",
    "# Checking the shape of the train and test sets\n",
    "# This will output the number of samples and features in both training and test datasets\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Th·ª±c hi·ªán chu·∫©n h√≥a (standardization) ƒë·∫∑c tr∆∞ng (features) trong t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm tra:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature scaling (Standardization)\n",
    "# We are using StandardScaler from sklearn to standardize the features (X).\n",
    "# This ensures that each feature has a mean of 0 and a standard deviation of 1.\n",
    "# The scaler is first fitted on the training data and then applied to both training and testing data.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)  # Fit the scaler to the training data and transform it\n",
    "x_test = scaler.transform(x_test)  # Apply the same transformation to the testing data\n",
    "\n",
    "# Checking the shape of the scaled data\n",
    "# This will output the number of samples and features in both the scaled training and testing datasets\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VI. X√¢y d·ª±ng m√¥ h√¨nh"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Logistic Regression kh√¥ng h·ªó tr·ª£ ƒë·∫ßu ra one-hot tr·ª±c ti·∫øp,\n",
    "# n√™n ta d√πng OneVsRestClassifier ƒë·ªÉ x·ª≠ l√Ω ph√¢n l·ªõp ƒëa nh√£n\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VII. Hu·∫•n luy·ªán m√¥ h√¨nh"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model.fit(x_train, y_train)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## VIII. ƒê√°nh gi√° m√¥ h√¨nh\n",
    "#### ƒê√°nh gi√° m√¥ h√¨nh tr√™n t·∫≠p ki·ªÉm tra v√† V·∫Ω ƒë·ªì th·ªã th·ªÉ hi·ªán qu√° tr√¨nh h·ªçc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "# The 'predict' method returns the predicted probabilities for each class.\n",
    "pred_test = model.predict(x_test)\n",
    "\n",
    "# Convert the predicted probabilities to the actual class labels using the encoder\n",
    "# 'inverse_transform' converts one-hot encoded predictions back to original labels\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "\n",
    "# Convert the actual labels (y_test) from one-hot encoding to original labels for comparison\n",
    "y_test_labels = encoder.inverse_transform(y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# T√≠nh ƒë·ªô ch√≠nh x√°c\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of our Logistic Regression model on test data: {:.2f}%\".format(accuracy * 100))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### So s√°nh tr·ª±c quan nh√£n d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh (Predicted Labels) v·ªõi nh√£n th·∫≠t c·ªßa d·ªØ li·ªáu test (Actual Labels)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a DataFrame to compare predicted labels with actual labels\n",
    "# 'y_pred' contains the predicted labels and 'y_test_labels' contains the true labels\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Predicted Labels': y_pred.flatten(),  # Flattening in case it's a multi-dimensional array\n",
    "    'Actual Labels': y_test_labels.flatten()  # Flattening to match the structure of 'Predicted Labels'\n",
    "})\n",
    "\n",
    "# Display the first 10 rows of the DataFrame to compare predictions with actual labels\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ma tr·∫≠n nh·∫ßm l·∫´n (confusion matrix)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "# Retrieve the list of labels from the encoder\n",
    "labels = encoder.categories_[0]  # This will give the emotions in the same order as the encoder\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pd.DataFrame(cm, index=labels, columns=labels),\n",
    "            linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='d')\n",
    "\n",
    "# Add titles and labels to the plot\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### B√°o c√°o ph√¢n lo·∫°i (classification report)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate and print the classification report\n",
    "print(classification_report(y_test_labels, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IX. Xu·∫•t m√¥ h√¨nh"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"emotion_model.joblib\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
