{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Nhận diện cảm xúc từ âm thanh\n",
    "Sử dụng Logistic Regression"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## I. Import thư viện cần thiết"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "# pandas and numpy for data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# os and sys for file and system path handling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# librosa is a library for audio processing, used here to extract features from audio files\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "# seaborn and matplotlib for data visualization (plots)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn libraries for preprocessing and splitting data\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# sklearn libraries for generating confusion matrix and detailed classification report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Seaborn library for visualization of the confusion matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# IPython's Audio class for playing audio files directly in Jupyter\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Suppress warnings to avoid cluttering the output\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## II. Chuẩn bị dữ liệu - Data Preparation\n",
    "Khai báo đường dẫn đến các tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define dataset paths\n",
    "\n",
    "# RAVDESS: Ryerson Audio-Visual Database of Emotional Speech and Song\n",
    "Ravdess = \"data/ravdess-emotional-speech-audio/audio_speech_actors_01-24/archive/\"\n",
    "\n",
    "# CREMA-D: Crowd-sourced Emotional Multimodal Actors Dataset\n",
    "Crema = \"data/cremad/AudioWAV/\"\n",
    "\n",
    "# TESS: Toronto Emotional Speech Set\n",
    "Tess = \"data/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n",
    "\n",
    "# SAVEE: Surrey Audio-Visual Expressed Emotion\n",
    "Savee = \"data/surrey-audiovisual-expressed-emotion-savee/ALL/\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     1. RAVDESS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of subdirectories in the RAVDESS dataset directory\n",
    "ravdess_directory_list = os.listdir(Ravdess)\n",
    "\n",
    "file_emotion = []  # List to store the emotion label for each audio file\n",
    "file_path = []  # List to store the full path of each audio file\n",
    "\n",
    "for dir in ravdess_directory_list:\n",
    "    if dir == '.DS_Store':  # Skip MacOS system file\n",
    "        continue\n",
    "    # Each subdirectory corresponds to an actor and contains audio files\n",
    "    actor = os.listdir(Ravdess + dir)\n",
    "    for file in actor:\n",
    "        part = file.split('.')[0]  # Remove the file extension (.wav)\n",
    "        part = part.split('-')  # Split the filename into parts based on RAVDESS naming convention\n",
    "\n",
    "        # The third element in the filename represents the emotion\n",
    "        file_emotion.append(int(part[2]))\n",
    "\n",
    "        # Construct and save the full path to the audio file\n",
    "        file_path.append(Ravdess + dir + '/' + file)\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the audio file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "Ravdess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Map numeric emotion codes to actual emotion labels\n",
    "Ravdess_df.Emotions.replace(\n",
    "    {\n",
    "        1: 'neutral',\n",
    "        2: 'calm',\n",
    "        3: 'happy',\n",
    "        4: 'sad',\n",
    "        5: 'angry',\n",
    "        6: 'fear',\n",
    "        7: 'disgust',\n",
    "        8: 'surprise'\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# Display the first few rows of the processed DataFrame\n",
    "Ravdess_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     2. CREMA-D"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of all audio files in the CREMA-D dataset directory\n",
    "crema_directory_list = os.listdir(Crema)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store file paths\n",
    "\n",
    "for file in crema_directory_list:\n",
    "    # Append the full file path to the list\n",
    "    file_path.append(Crema + file)\n",
    "\n",
    "    # Extract emotion code from the file name (e.g., '1001_DFA_ANG_XX.wav')\n",
    "    part = file.split('_')\n",
    "\n",
    "    # Map the emotion code to the corresponding emotion label\n",
    "    if part[2] == 'SAD':\n",
    "        file_emotion.append('sad')\n",
    "    elif part[2] == 'ANG':\n",
    "        file_emotion.append('angry')\n",
    "    elif part[2] == 'DIS':\n",
    "        file_emotion.append('disgust')\n",
    "    elif part[2] == 'FEA':\n",
    "        file_emotion.append('fear')\n",
    "    elif part[2] == 'HAP':\n",
    "        file_emotion.append('happy')\n",
    "    elif part[2] == 'NEU':\n",
    "        file_emotion.append('neutral')\n",
    "    else:\n",
    "        file_emotion.append('Unknown')  # Fallback in case of unexpected code\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine emotion and path into one DataFrame\n",
    "Crema_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "Crema_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     3. TESS"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of speaker folders in the TESS dataset directory\n",
    "tess_directory_list = os.listdir(Tess)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store full file paths\n",
    "\n",
    "# Iterate through each speaker's folder\n",
    "for dir in tess_directory_list:\n",
    "    directories = os.listdir(Tess + dir)  # Get list of audio files in the speaker folder\n",
    "    for file in directories:\n",
    "        # Extract the emotion from the file name (e.g., 'OAF_angry.wav')\n",
    "        part = file.split('.')[0]\n",
    "        part = part.split('_')[2]  # The third segment contains the emotion label\n",
    "\n",
    "        # Handle special case: 'ps' stands for 'pleasant surprise'\n",
    "        if part == 'ps':\n",
    "            file_emotion.append('surprise')\n",
    "        else:\n",
    "            file_emotion.append(part)\n",
    "\n",
    "        # Append the full file path\n",
    "        file_path.append(Tess + dir + '/' + file)\n",
    "\n",
    "# Create DataFrame for extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create DataFrame for corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine both into a single DataFrame\n",
    "Tess_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Preview the result\n",
    "Tess_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "###     4. SAVEE"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Get the list of all audio files in the SAVEE dataset directory\n",
    "savee_directory_list = os.listdir(Savee)\n",
    "\n",
    "file_emotion = []  # List to store emotion labels\n",
    "file_path = []  # List to store full file paths\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for file in savee_directory_list:\n",
    "    file_path.append(Savee + file)  # Construct full file path\n",
    "\n",
    "    # Extract the emotion code from the filename\n",
    "    part = file.split('_')[1]\n",
    "    ele = part[:-6]  # Remove the numerical part and extension\n",
    "\n",
    "    # Map short emotion codes to full labels\n",
    "    if ele == 'a':\n",
    "        file_emotion.append('angry')\n",
    "    elif ele == 'd':\n",
    "        file_emotion.append('disgust')\n",
    "    elif ele == 'f':\n",
    "        file_emotion.append('fear')\n",
    "    elif ele == 'h':\n",
    "        file_emotion.append('happy')\n",
    "    elif ele == 'n':\n",
    "        file_emotion.append('neutral')\n",
    "    elif ele == 'sa':\n",
    "        file_emotion.append('sad')\n",
    "    else:\n",
    "        file_emotion.append('surprise')\n",
    "\n",
    "# Create a DataFrame for the extracted emotions\n",
    "emotion_df = pd.DataFrame(file_emotion, columns=['Emotions'])\n",
    "\n",
    "# Create a DataFrame for the corresponding file paths\n",
    "path_df = pd.DataFrame(file_path, columns=['Path'])\n",
    "\n",
    "# Combine the two DataFrames into one\n",
    "Savee_df = pd.concat([emotion_df, path_df], axis=1)\n",
    "\n",
    "# Preview the result\n",
    "Savee_df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### => Kết hợp các tập dữ liệu đã tải"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine datasets from all four sources (RAVDESS, CREMA-D, TESS, SAVEE)\n",
    "data_path = pd.concat([Ravdess_df, Crema_df, Tess_df, Savee_df], axis=0)\n",
    "\n",
    "# Xoá trộn tránh bias\n",
    "data_path = data_path.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save the combined DataFrame to a CSV file for future use\n",
    "data_path.to_csv(\"data_path.csv\", index=False)\n",
    "\n",
    "# Preview the first few rows of the combined dataset\n",
    "data_path.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 📊Biểu đồ số lượng mẫu cho mỗi cảm xúc"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plotting the number of samples for each emotion\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # Set figure size for better readability\n",
    "\n",
    "# Create a count plot for the 'Emotions' column\n",
    "sns.countplot(x='Emotions', data=data_path, palette='muted')\n",
    "\n",
    "# Rotate x-axis labels for better visibility\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Count of Emotions', fontsize=16)\n",
    "plt.xlabel('Emotions', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "\n",
    "# Remove top and right borders for a cleaner look\n",
    "sns.despine(top=True, right=True, left=False, bottom=False)\n",
    "\n",
    "# Automatically adjust subplot parameters to fit the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the final plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## III. Khám phá và trực quan dữ liệu - (Exploratory Data Analysis - EDA):\n",
    "##### 1. Biểu đồ dạng sóng (waveform).\n",
    "##### 2. Quang phổ âm thanh (spectrogram) - biểu diễn tín hiệu âm thanh trong miền tần số theo thời gian."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to create a waveplot for a given emotion and audio data\n",
    "def create_waveplot(data, sr, e):\n",
    "    plt.figure(figsize=(10, 3))  # Set figure size\n",
    "    plt.title(f'Waveplot for audio with {e} emotion', size=15)  # Title of the plot\n",
    "    plt.plot(np.linspace(0, len(data) / sr, num=len(data)), data, alpha=0.7)  # Plot audio data over time\n",
    "    plt.xlabel(\"Time (s)\")  # X-axis label\n",
    "    plt.ylabel(\"Amplitude\")  # Y-axis label\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    plt.show()  # Display the plot\n",
    "\n",
    "\n",
    "# Function to create a spectrogram for a given emotion and audio data\n",
    "def create_spectrogram(data, sr, e):\n",
    "    # Compute the Short-Time Fourier Transform (STFT)\n",
    "    X = librosa.stft(data)\n",
    "    # Convert amplitude to decibels\n",
    "    Xdb = librosa.amplitude_to_db(np.abs(X))\n",
    "    plt.figure(figsize=(12, 3))  # Set figure size\n",
    "    plt.title(f'Spectrogram for audio with {e} emotion', size=15)  # Title of the plot\n",
    "    # Display the spectrogram with the amplitude in decibels\n",
    "    librosa.display.specshow(Xdb, sr=sr, x_axis='time', y_axis='hz', cmap='magma')\n",
    "    plt.colorbar(format=\"%+2.0f dB\")  # Add color bar\n",
    "    plt.tight_layout()  # Adjust layout for better fit\n",
    "    plt.show()  # Display the plot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def visualize_emotion(emotion, data_path):\n",
    "    try:\n",
    "        # Lấy file đầu tiên có emotion tương ứng\n",
    "        path = np.array(data_path.Path[data_path.Emotions == emotion])[0]\n",
    "\n",
    "        # Load audio\n",
    "        data, sampling_rate = librosa.load(path, sr=None)\n",
    "\n",
    "        # Vẽ sóng âm thanh\n",
    "        create_waveplot(data, sampling_rate, emotion)\n",
    "\n",
    "        # Vẽ spectrogram\n",
    "        create_spectrogram(data, sampling_rate, emotion)\n",
    "\n",
    "        # Phát audio trong notebook\n",
    "        display(Audio(path))\n",
    "\n",
    "    except IndexError:\n",
    "        print(f\"No audio files found for emotion: {emotion}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "visualize_emotion('happy', data_path)\n",
    "\n",
    "visualize_emotion('sad', data_path)\n",
    "\n",
    "visualize_emotion('fear', data_path)\n",
    "\n",
    "visualize_emotion('angry', data_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IV. Mở rộng dữ liêu - Data Augmentation"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Data Augmentation functions\n",
    "\n",
    "def noise(data):\n",
    "    # Generate random noise amplitude based on the maximum value of the data\n",
    "    noise_amp = 0.035 * np.random.uniform() * np.amax(data)  # Random amplitude of noise\n",
    "    # Add Gaussian noise to the original signal\n",
    "    data = data + noise_amp * np.random.normal(size=data.shape[0])  # Apply noise to the audio data\n",
    "    return data\n",
    "\n",
    "\n",
    "def stretch(data, rate=0.8):\n",
    "    # Ensure that data is a numpy array and of type float32 for time-stretching\n",
    "    return librosa.effects.time_stretch(np.array(data).astype(np.float32),\n",
    "                                        rate=rate)  # Stretch the audio signal in time\n",
    "\n",
    "\n",
    "def shift(data):\n",
    "    # Generate a random shift range for shifting the audio signal in time\n",
    "    shift_range = int(np.random.uniform(low=-5, high=5) * 1000)  # Random shift in milliseconds\n",
    "    # Shift the audio data by the computed range using np.roll\n",
    "    return np.roll(data, shift_range)  # Shift the audio data in time\n",
    "\n",
    "\n",
    "def pitch(data, sampling_rate, pitch_factor=0.7):\n",
    "    # Ensure that data is a numpy array and of type float32 for pitch shifting\n",
    "    data = np.array(data)\n",
    "    # Apply pitch shift by a factor (in steps), which will change the pitch of the audio signal\n",
    "    return librosa.effects.pitch_shift(data.astype(np.float32), sr=sampling_rate, n_steps=pitch_factor)\n",
    "\n",
    "\n",
    "# Taking any example from the dataset and testing the augmentation functions\n",
    "path = np.array(data_path.Path)[1]  # Example audio file path\n",
    "data, sample_rate = librosa.load(path, sr=None)  # Load the audio file and obtain its data and sample rate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. Ví dụ đơn giản trực quan hoá âm thanh theo dạng sóng\n",
    "-> Kiểm tra và đánh giá chất lượng dữ liệu đầu vào."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simple audio waveform visualization\n",
    "\n",
    "# Create a figure with a specified size\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the audio signal data as a waveform.\n",
    "# `np.linspace` is used to create time values based on the number of samples and the sample rate.\n",
    "plt.plot(np.linspace(0, len(data) / sample_rate, num=len(data)), data, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title(\"Waveform\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\"\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\"\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Automatically adjust the layout to fit everything properly\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the audio file using IPython's Audio class\n",
    "Audio(path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 2. Noise Injection Visualization (thêm nhiễu vào tín hiệu gốc)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Noise Injection: Adding random noise to the audio signal\n",
    "\n",
    "# Inject noise into the original audio data\n",
    "x = noise(data)\n",
    "\n",
    "# Create a figure for plotting the noisy signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the noisy signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot\n",
    "plt.title(\"Noise Injection\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the noisy audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3. Time Stretching Visualization - Thay đổi tốc độ phát âm mà không làm thay đổi cao độ (pitch)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time Stretching: Changing the speed of the audio signal without altering its pitch\n",
    "\n",
    "# Apply time-stretching to the original audio data\n",
    "x = stretch(data)\n",
    "\n",
    "# Create a figure for plotting the stretched signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the stretched signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for time-stretching\n",
    "plt.title(\"Time Stretching\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the time-stretched audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 4.Time Shifting Visualization – Dịch chuyển tín hiệu âm thanh theo thời gian."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Time Shifting: Shifting the audio signal in time (delays or advances the signal)\n",
    "\n",
    "# Apply time-shifting to the original audio data\n",
    "x = shift(data)\n",
    "\n",
    "# Create a figure for plotting the shifted signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the shifted signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for time-shifting\n",
    "plt.title(\"Time Shifting\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the time-shifted audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 5. Pitch Shifting Visualization — Thay đổi cao độ (pitch) của tín hiệu âm thanh mà không thay đổi tốc độ phát."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Pitch Shifting: Changing the pitch (frequency) of the audio signal\n",
    "\n",
    "# Apply pitch-shifting to the original audio data\n",
    "x = pitch(data, sample_rate)\n",
    "\n",
    "# Create a figure for plotting the pitch-shifted signal\n",
    "plt.figure(figsize=(14, 4))\n",
    "\n",
    "# Plot the pitch-shifted signal as a waveform. `np.linspace` is used to create time values.\n",
    "plt.plot(np.linspace(0, len(x) / sample_rate, num=len(x)), x, alpha=0.7)\n",
    "\n",
    "# Set the title of the plot to indicate it's for pitch-shifting\n",
    "plt.title(\"Pitch Shifting\")\n",
    "\n",
    "# Label the x-axis as \"Time (s)\" to indicate time duration\n",
    "plt.xlabel(\"Time (s)\")\n",
    "\n",
    "# Label the y-axis as \"Amplitude\" to indicate the amplitude of the audio signal\n",
    "plt.ylabel(\"Amplitude\")\n",
    "\n",
    "# Adjust the layout to avoid overlapping labels\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "# Play the pitch-shifted audio using IPython's Audio class\n",
    "Audio(x, rate=sample_rate)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## V. Trích xuất đặc trưng - Feature Extraction"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature Extraction (updated for newer APIs)\n",
    "def extract_features(data):\n",
    "    \"\"\"\n",
    "    This function extracts various audio features from the given audio data.\n",
    "    Features include zero-crossing rate (ZCR), chroma_stft, MFCC, RMS, and mel spectrogram.\n",
    "    \"\"\"\n",
    "    result = np.array([])  # Initialize an empty array to store features\n",
    "\n",
    "    # Zero-Crossing Rate (ZCR): Measure of how many times the signal changes sign\n",
    "    zcr = np.mean(librosa.feature.zero_crossing_rate(y=data).T, axis=0)\n",
    "    result = np.hstack((result, zcr))  # Append the ZCR feature to result array\n",
    "\n",
    "    # Chroma Short-Time Fourier Transform (chroma_stft): Measures harmonic and melodic content\n",
    "    stft = np.abs(librosa.stft(data))  # Short-time Fourier transform of the audio signal\n",
    "    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T, axis=0)  # Compute chroma features\n",
    "    result = np.hstack((result, chroma_stft))  # Append the chroma feature to result array\n",
    "\n",
    "    # Mel-frequency Cepstral Coefficients (MFCC): Represent the short-term power spectrum of sound\n",
    "    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=13).T, axis=0)  # Extract 13 MFCCs\n",
    "    result = np.hstack((result, mfcc))  # Append the MFCC feature to result array\n",
    "\n",
    "    # Root Mean Square (RMS): Energy of the audio signal\n",
    "    rms = np.mean(librosa.feature.rms(y=data).T, axis=0)  # Calculate RMS value\n",
    "    result = np.hstack((result, rms))  # Append the RMS feature to result array\n",
    "\n",
    "    # Mel Spectrogram: Spectrogram representation of the audio signal using Mel scale\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=data, sr=sample_rate).T, axis=0)  # Extract Mel spectrogram\n",
    "    result = np.hstack((result, mel))  # Append the Mel spectrogram feature to result array\n",
    "\n",
    "    return result  # Return the array of extracted features\n",
    "\n",
    "\n",
    "def get_features(path):\n",
    "    \"\"\"\n",
    "    This function loads the audio data from the given path, extracts features from the original data,\n",
    "    and applies data augmentation techniques (noise, stretching, and pitch shifting) to generate additional features.\n",
    "    \"\"\"\n",
    "    # Load the audio file (duration and offset are specified to load a portion of the audio)\n",
    "    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "\n",
    "    # Base features: Extract features from the original audio data\n",
    "    res1 = extract_features(data)\n",
    "    result = np.array(res1)  # Store base features as the result\n",
    "\n",
    "    # Noise Augmentation: Apply noise injection and extract features from the noisy audio\n",
    "    noise_data = noise(data)\n",
    "    res2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, res2))  # Append noisy features to the result\n",
    "\n",
    "    # Stretching and Pitch Augmentation: Apply time-stretching and pitch-shifting, then extract features\n",
    "    new_data = stretch(data)  # Apply time-stretching\n",
    "    data_stretch_pitch = pitch(new_data, sample_rate)  # Apply pitch shifting\n",
    "    res3 = extract_features(data_stretch_pitch)  # Extract features from the stretched and pitch-shifted data\n",
    "    result = np.vstack((result, res3))  # Append augmented features to the result\n",
    "\n",
    "    return result  # Return the final feature set as a 2D array"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Trích xuất đặc trưng cho toàn bộ dataset âm thanh kèm theo nhãn cảm xúc tương ứng"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract features for the entire dataset\n",
    "X, Y = [], []  # Initialize lists to hold features (X) and labels (Y)\n",
    "\n",
    "# Loop through each audio file and its associated emotion\n",
    "for path, emotion in zip(data_path.Path, data_path.Emotions):\n",
    "\n",
    "    # Extract features for the current audio file (including augmented variants)\n",
    "    features = get_features(path)  # Extract features for the original audio and its augmented versions\n",
    "\n",
    "    # Loop through the extracted features (original, noisy, stretched + pitched) and append them to X\n",
    "    for feature in features:\n",
    "        X.append(feature)  # Append the feature to the feature list (X)\n",
    "        Y.append(emotion)  # Append the corresponding emotion label to the label list (Y)\n",
    "\n",
    "        # Repeat 3 times because we have 3 variants (original, noise, and stretch + pitch)\n",
    "        # This ensures that for each audio file, we have 3 corresponding entries in X and Y"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Kiểm tra số: Mẫu đặc trưng (X) và số nhãn tương ứng với các mẫu (Y)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Check the length of the feature set (X), the labels (Y), and the number of paths in data_path\n",
    "len(X), len(Y), data_path.Path.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Lưu trữ dữ liệu đặc trưng và nhãn của toàn bộ dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the features and labels into a DataFrame\n",
    "Features = pd.DataFrame(X)\n",
    "Features['labels'] = Y\n",
    "\n",
    "# Save to a CSV file\n",
    "Features.to_csv('features.csv', index=False)\n",
    "\n",
    "# Display the first few rows\n",
    "Features.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chuẩn bị dữ liệu đặc trưng và nhãn"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Features\n",
    "X = Features.iloc[:, :-1].values  # Extract all columns except the last (labels)\n",
    "\n",
    "# Labels\n",
    "Y = Features['labels'].values  # Extract the labels column"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Mã hóa nhãn dạng phân loại thành dạng vector one-hot (chuẩn hóa cho bài toán phân loại đa lớp)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# One-hot Encoding for labels\n",
    "# As this is a multiclass classification problem, we are one-hot encoding the labels (Y).\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Convert labels (Y) to a 2D array (required by OneHotEncoder) and apply fit_transform.\n",
    "# This will convert each categorical label into a binary vector (one-hot encoded).\n",
    "Y = encoder.fit_transform(np.array(Y).reshape(-1, 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Chia dữ liệu thành tập huấn luyện và tập kiểm tra"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Splitting data into training and testing sets\n",
    "# We use train_test_split from scikit-learn to split the data (X) and labels (Y)\n",
    "# into training and testing sets. The random_state ensures reproducibility of the split.\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\n",
    "\n",
    "# Checking the shape of the train and test sets\n",
    "# This will output the number of samples and features in both training and test datasets\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Thực hiện chuẩn hóa (standardization) đặc trưng (features) trong tập huấn luyện và tập kiểm tra:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Feature scaling (Standardization)\n",
    "# We are using StandardScaler from sklearn to standardize the features (X).\n",
    "# This ensures that each feature has a mean of 0 and a standard deviation of 1.\n",
    "# The scaler is first fitted on the training data and then applied to both training and testing data.\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)  # Fit the scaler to the training data and transform it\n",
    "x_test = scaler.transform(x_test)  # Apply the same transformation to the testing data\n",
    "\n",
    "# Checking the shape of the scaled data\n",
    "# This will output the number of samples and features in both the scaled training and testing datasets\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VI. Xây dựng mô hình"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# Logistic Regression không hỗ trợ đầu ra one-hot trực tiếp,\n",
    "# nên ta dùng OneVsRestClassifier để xử lý phân lớp đa nhãn\n",
    "model = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='lbfgs'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## VII. Huấn luyện mô hình"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "model.fit(x_train, y_train)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## VIII. Đánh giá mô hình\n",
    "#### Đánh giá mô hình trên tập kiểm tra và Vẽ đồ thị thể hiện quá trình học"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "# The 'predict' method returns the predicted probabilities for each class.\n",
    "pred_test = model.predict(x_test)\n",
    "\n",
    "# Convert the predicted probabilities to the actual class labels using the encoder\n",
    "# 'inverse_transform' converts one-hot encoded predictions back to original labels\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "\n",
    "# Convert the actual labels (y_test) from one-hot encoding to original labels for comparison\n",
    "y_test_labels = encoder.inverse_transform(y_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Tính độ chính xác\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy of our Logistic Regression model on test data: {:.2f}%\".format(accuracy * 100))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### So sánh trực quan nhãn dự đoán của mô hình (Predicted Labels) với nhãn thật của dữ liệu test (Actual Labels)."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Create a DataFrame to compare predicted labels with actual labels\n",
    "# 'y_pred' contains the predicted labels and 'y_test_labels' contains the true labels\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Predicted Labels': y_pred.flatten(),  # Flattening in case it's a multi-dimensional array\n",
    "    'Actual Labels': y_test_labels.flatten()  # Flattening to match the structure of 'Predicted Labels'\n",
    "})\n",
    "\n",
    "# Display the first 10 rows of the DataFrame to compare predictions with actual labels\n",
    "df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Ma trận nhầm lẫn (confusion matrix)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test_labels, y_pred)\n",
    "\n",
    "# Retrieve the list of labels from the encoder\n",
    "labels = encoder.categories_[0]  # This will give the emotions in the same order as the encoder\n",
    "\n",
    "# Plot the confusion matrix as a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(pd.DataFrame(cm, index=labels, columns=labels),\n",
    "            linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='d')\n",
    "\n",
    "# Add titles and labels to the plot\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Báo cáo phân loại (classification report)"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Generate and print the classification report\n",
    "print(classification_report(y_test_labels, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## IX. Xuất mô hình"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(model, \"emotion_model.joblib\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
